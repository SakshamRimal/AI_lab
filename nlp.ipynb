{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJB6X1D5eAK70Kf7B9+Fd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimalsaksham07/AI_lab/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG2WHGnCIl-p",
        "outputId": "0ac0844e-68cb-452f-bc2d-8d110a32d993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['Artificial intelligence and machine learning are revolutionizing industries across the globe.']\n",
            "Word Tokenization:\n",
            "['Artificial', 'intelligence', 'and', 'machine', 'learning', 'are', 'revolutionizing', 'industries', 'across', 'the', 'globe', '.']\n",
            "Filtered List (without stopwords):\n",
            "['Artificial', 'intelligence', 'machine', 'learning', 'revolutionizing', 'industries', 'across', 'globe', '.']\n",
            "Sentiment Analysis Scores:\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Named Entities:\n",
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  launched/VBD\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  with/IN\n",
            "  the/DT\n",
            "  goal/NN\n",
            "  of/IN\n",
            "  colonizing/VBG\n",
            "  (PERSON Mars/NNP)\n",
            "  ./.)\n",
            "Frequency Distribution Most Common Words:\n",
            "[(',', 2), ('will', 2), ('In', 1), ('the', 1), ('future', 1), ('self-driving', 1), ('cars', 1), ('be', 1), ('a', 1), ('common', 1)]\n",
            "Accuracy of Classifier: 0.714\n",
            "Classification of New Review: pos\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords, movie_reviews\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk import FreqDist\n",
        "import random\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('averaged_perceptron_tagger') # Download the missing resource\n",
        "\n",
        "# New input text for tokenization and analysis\n",
        "example_string = \"Artificial intelligence and machine learning are revolutionizing industries across the globe.\"\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sent_tokenize(example_string))\n",
        "\n",
        "print(\"Word Tokenization:\")\n",
        "words_in_example = word_tokenize(example_string)\n",
        "print(words_in_example)\n",
        "\n",
        "# Stop words removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = [word for word in words_in_example if word.casefold() not in stop_words]\n",
        "print(\"Filtered List (without stopwords):\")\n",
        "print(filtered_list)\n",
        "\n",
        "# Sentiment Analysis on a new sentence\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_text = \"AI is rapidly becoming an integral part of modern technology.\"\n",
        "print(\"Sentiment Analysis Scores:\")\n",
        "print(sia.polarity_scores(sentiment_text))\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "ner_text = \"Elon Musk launched SpaceX with the goal of colonizing Mars.\"\n",
        "words_ner = word_tokenize(ner_text)\n",
        "tagged_words = pos_tag(words_ner)\n",
        "named_entities = ne_chunk(tagged_words)\n",
        "print(\"Named Entities:\")\n",
        "print(named_entities)\n",
        "\n",
        "# Frequency Distribution for new text\n",
        "frq_sen = \"\"\"In the future, self-driving cars will be a common sight on roads, and autonomous systems will manage various aspects of our daily lives.\"\"\"\n",
        "words_freq = word_tokenize(frq_sen)\n",
        "frequency_distribution = FreqDist(words_freq)\n",
        "print(\"Frequency Distribution Most Common Words:\")\n",
        "print(frequency_distribution.most_common(10))\n",
        "\n",
        "# Sentiment classifier using movie reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "for category in movie_reviews.categories()\n",
        "for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "def extract_features(words):\n",
        "    return {word: True for word in words}\n",
        "\n",
        "featuresets = [(extract_features(words), category) for (words, category) in documents]\n",
        "train_set, test_set = featuresets[:1500], featuresets[1500:]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(\"Accuracy of Classifier:\", accuracy)\n",
        "\n",
        "# Classification of a new review\n",
        "new_review = \"The storyline was gripping and the visual effects were stunning.\"\n",
        "new_features = extract_features(word_tokenize(new_review))\n",
        "print(\"Classification of New Review:\", classifier.classify(new_features))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDxFrqEwIqTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}